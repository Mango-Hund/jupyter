{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于概率论的分类方法:朴素贝叶斯\n",
    "\n",
    "优点: 在数据较少的情况下仍然有效,可以处理多类问题\n",
    "缺点: 对于输入数据的准备方式较为敏感\n",
    "适用数据类型:标称性数据\n",
    "\n",
    "贝叶斯决策理论:  \n",
    "用p1(x,y)来表示数据点(x,y)属于类别1的概率,用p2(x,y)表示数据点(x,y)属于类别2的概率,那么对于一个新数据点(x,y),可以用下面的规则来判断它的类别:  \n",
    "* 如果$p_1(x,y)>p_2(x,y)$,那么类别为1.\n",
    "* 如果$p_2(x,y)>p_1(x,y)$,那么类别为2. \n",
    "\n",
    "也就是说我们会选择高概率对应的类别.这就是贝叶斯决策理论的核心思想,即选择具有最高概率的决策."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用条件概率来分类\n",
    "真正需要计算和比较的是$p(c_1|x,y)$和$p(c_2|x,y)$.这些符号所代表的具体意义是:给定某个由x,y表示的数据点,那么该数据点来自类别$c_1$的概率是多少呢?数据点来自类别$c_2$的概率又是多少?注意这些概率与$p(x,y|c_1)$并不一样,不过可以使用贝叶斯准则来交换概率中条件与结果.具体的,应用贝叶斯准则得到:\n",
    "$$p(c_i|x,y)=\\frac{p(x,y|c_i)p(c_i)}{p(x,y)} $$ \n",
    "* 如果$p(c_1|x,y)>p(c_2|x,y)$,那么属于类别c_1\n",
    "* 如果$p(c_2|x,y)>p(c_1|x,y)$,那么属于类别c_2\n",
    "\n",
    "使用贝叶斯准则,可以通过已知的三个概率值来计算未知的概率值."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用朴素贝叶斯进行文档分类\n",
    "机器学习的一个重要应用就是文档的自动分类.  \n",
    "朴素贝叶斯的一般过程:  \n",
    "1. 收集数据:可以使用任何方法.\n",
    "2. 准备数据:需要数值型或者布尔型数据.\n",
    "3. 分析数据:有大量特征时,绘制特征 作用不大,此时使用直方图效果更好\n",
    "4. 训练算法:计算不同的独立特征的条件概率\n",
    "5. 测试算法:计算错误率\n",
    "6. 使用算法:一个常见的朴素贝叶斯应用是文档分类.可以在任意的分类场景中使用朴素贝叶斯分类器,不一定非要是文本.\n",
    "\n",
    "朴素贝叶斯分类器的一个假设是:每个特征同等重要."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Python进行文本分类\n",
    "要从文本中获取特征,需要先拆分文本. 这里的特征是来自文本的词条(token),一个词条是字符的任意组合.然后将每一个文本片段表示为一个词条向量,其中值为1表示词条出现在文档中,0表示未出现.  \n",
    "以在线社区的留言板为例。为了不影响社区的发展，我们要屏蔽侮辱性的言论，所以要构建一个快速过滤器，如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标识为内容不当。过滤这类内容是一个很常见的需求。对此问题建立两个类别：侮辱类和非侮辱类，使用1和0分别表示。\n",
    "### 准备数据:从文本中构建词向量\n",
    "我们将把文本看成单词向量或者词条向量，也就是说将句子转换为向量。考虑出现在所有文档中的所有单词，再决定将哪些词纳入词汇表或者说所要的词汇集合，然后必须要将每一篇文档转换为词汇表上的向量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T03:36:04.689812Z",
     "start_time": "2020-08-07T03:36:04.682047Z"
    }
   },
   "outputs": [],
   "source": [
    "def loadDataSet():      #创建一些实验样本,该\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid','worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate' 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit','buying', 'worthless', 'dog', 'food','stupid']]\n",
    "    classVec = [0,1,0,1,0,1] #1代表侮辱性文字,0代表正常言论\n",
    "    return postingList,classVec\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])     #set() 函数创建一个无序不重复元素集，可进行关系测试，删除重复数据，还可以计算交集、差集、并集等\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1   #在词条出现的位置标1\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T03:37:08.780579Z",
     "start_time": "2020-08-07T03:37:08.770514Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flea',\n",
       " 'him',\n",
       " 'problems',\n",
       " 'is',\n",
       " 'cute',\n",
       " 'stop',\n",
       " 'worthless',\n",
       " 'love',\n",
       " 'take',\n",
       " 'how',\n",
       " 'my',\n",
       " 'mr',\n",
       " 'stupid',\n",
       " 'atemy',\n",
       " 'quit',\n",
       " 'not',\n",
       " 'I',\n",
       " 'maybe',\n",
       " 'park',\n",
       " 'dalmation',\n",
       " 'please',\n",
       " 'food',\n",
       " 'so',\n",
       " 'garbage',\n",
       " 'to',\n",
       " 'steak',\n",
       " 'licks',\n",
       " 'has',\n",
       " 'dog',\n",
       " 'buying',\n",
       " 'posting',\n",
       " 'help']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOPosts,listClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOPosts)\n",
    "myVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T03:38:34.377325Z",
     "start_time": "2020-08-07T03:38:34.372425Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList, listOPosts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T03:43:48.580370Z",
     "start_time": "2020-08-07T03:43:48.575497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']\n"
     ]
    }
   ],
   "source": [
    "print(listOPosts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T03:46:47.279759Z",
     "start_time": "2020-08-07T03:46:47.273899Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList, listOPosts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练算法:从词向量计算概率\n",
    "现在已经知道了一个词是否出现在一篇文档中,也知道该文档所属的类别.我们重写贝叶斯准则,将之前的x,y替换为**w**.粗体**w**表示这是一个向量,即它由多个数值组成.在这个例子中,数值个数与词汇表中的词个数相同.\n",
    "$$p(c_i|w)=\\frac{p(w|c_i)p(c_i)}{p(w)}$$\n",
    "我们将使用上述公式,对每个类计算该值,然后比较这两个概率值的大小.首先可以通过类别i(侮辱性留言或非侮辱性留言)中文档数除以总的文档数来计算$p(c_i)$.接下来计算$p(w|c_i)$,这里就要用到朴素贝叶斯假设.如果将w展开为一个个独立特征,那么就可以将上述概率写作$p(w_0,w_1,w_2..w_n|c_i)$.这里假设所有词都互相独立,该假设也称作条件独立性假设,它意味着可以使用$p(w_0|c_i)p(w_1|c_i)...p(w_n|c_i)$来计算上述概率.  \n",
    "该函数的伪代码如下:  \n",
    "```\n",
    "计算每个类别中的文档数目\n",
    "对每篇训练文档:\n",
    "   对每个类别:\n",
    "       如果词条出现在文档中->增加该词条的计数值\n",
    "       增加所有词条的计数值\n",
    "   对每个类别:\n",
    "       对每个词条:\n",
    "           将该词条的数目除以总词条数目得到条件概率\n",
    "   返回每个类别的条件概率\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T04:11:32.244716Z",
     "start_time": "2020-08-07T04:11:31.276373Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = zeros(numWords); p1Num = zeros(numWords)\n",
    "    p0Denom = 0.0; p1Denom = 0.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = p1Num/p1Denom  #change to log()\n",
    "    p0Vect = p0Num/p0Denom  #change to log()\n",
    "    return p0Vect,p1Vect,PAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
